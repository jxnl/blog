 

Introduction and Course Overview

Alright, welcome back to session two of systematically improving your RAC applications. If you're not fine tuning, you're Blockbuster, not Netflix. And this is something I've said to a lot of different consulting clients in the past.

And the general idea here is that the goal isn't to fine tune a language model, right? Those are pretty hard and pretty expensive to train and require a lot of expertise. The goal is to train and fine tune embedding models, ones that move towards your  📍  📍 datasets and, improve retrieval and not generation.

So let's also look at our course overview.

For  📍  📍 the first half we're going to review the foundation and the fundamental mindset and engineering and behavioral shifts we need to make the RAG playbook work. Generate synthetic data, have fast evaluations, use those evals to start fine tuning this session and next session, we'll  📍  📍 talk how to build a UI that feels fast and collects data for us, allowing us to create  📍  📍 more examples .

In the second half, we're going to get deeper into optimizing your RAG systems. We're going to segment input queries to identify new topics and capabilities that we need  📍  📍 to invest in a very data driven manner,  refine and improve the query routing systems, and really think about exploring new  indices and new metadata  📍  📍 to  better index the data we have to answer the questions that 📍  📍  really matter.



Today we're going to go over the challenges with using third party providers existing embedding models, how we can think about improving our representations, and jump straight into the homework for this session and  📍  📍 what we're going to talk about next session. This should be pretty fast. 

Understanding Embedding Models

So most of us here should have a general understanding of what an embedding model is.

It takes a string, passes it through an embedding model, and it allows us to create a vector that we can use to represent the data 📍  📍  and the big assumption here , is that the embedding of the query vector should be similar to the embedding of a text chunk that just happens to have the answer

but it's a very big assumption and fundamentally, there's actually no consensus let me give you an example. Off the shelf embeddings do not really understand things like user intent and context. 📍  📍  And you tend to need more  📍  📍 metadata filters in real life to build a product.

And  📍  📍 so let me jump into an example that I've been thinking about for basically four years of my life. What would a similar product look like?  So I used to do a lot of image based recommendation  📍  📍 systems, so now ,imagine a query  📍  📍 of a red shirt. 

Is the similar mean just more similar red shirts? Does it matter if I'm looking  📍  📍 at a silk shirt or does it matter that I'm looking at a 20 polyester shirt? Are shirts more similar if they have different?  📍  📍 Crew  📍  📍 necks , but the colors are different,  or should we be thinking about pants, shoes, and bags that compliment this red shirt as part of an outfit?

All of these could be correct, and all of these could be incorrect, but they fundamentally go back down to the simple idea  📍  📍 that  the quality of your multimodal embedding depends on the objective function that you use to fit and train these embedding models.

 

Challenges with Third-Party Embedding Models

Why does implementing a third party provider's existing model fail, right? The assumption here, and the wrong assumption here. is that cosine distance will actually give us what we want. For example, if we think of a rag use case, the assumption is that the embedding of the query is going to be similar to the embedding of the text chunk. 

And that destined that, and that distance is going to reflect some kind of relevance.

If we think about an e commerce setting, the assumption is that the embedding of the user and the embedding of the product

are going to be similar. And the distance between the two might be the probability of a purchase.

But what about the embedding of a product compared to the embedding of a product? Do we want to define them as complementary goods? As substitutable goods? Should they be recommending other things we can add into a cart? It's really not specified same with  the embedding of a song versus the embedding of a user of Spotify.

Is it the probability that the user will listen to the song? Is it the probability that they will add it to their playlist or they'll heart it? Again, not really sure.  We can even go further. Imagine the embedding of a song compared to the embedding of another song. Does the distance correlate with whether or not they're in the same playlist or if they're liked by the same people or if they have the same style or the BPM?

We realize that  all of what we're doing here is we're assuming 📍  📍  what the distance function really tries to correlate with without actually having the ability to train a model that just points at exactly what we care about. We can train a model that says two songs are similar if they're in the playing list.

We can train a model if they're the same style. We can train a model if they're played by the same people. But we have to decide what  📍  📍 that objective function really is. 

Using a third party provider's embedding model bakes many assumptions on what similarity means. And that definition is baked into the dataset that we use to train it. And if you don't know what it is, and if you don't know what the data set they're using is, you're not going to know what similarity means.

So let's go with a really straightforward example for a dating app. Maybe you're building like a hinge or a tinder.

Maybe you ask yourselves, Okay, someone's bio says I love coffee, and someone else's bio says I hate coffee. Should I love coffee or I hate coffee be similar or different? From a linguistics perspective, it might be different because they're obviously negations of each other, but maybe they're all just preferences.

Maybe they're different because people who love coffee would not date people who hate coffee. Maybe they're similar because all of this actually specifies that  they are foodies. This might demonstrate some strong preferences for foods and drinks and whatnot.

And maybe as long as one loves tea and one drinks coffee,  📍  📍 they'll actually get along. But again, who really knows? But really what's happening is that these questions don't actually matter in isolation. Even if OpenAI Embeddings has data on comparing  texts,  the objective function is very different based on the product you're building.

And ultimately, what we actually care about building in a dating app is whether or not two profiles, when you compare them, predicts whether they will like each other,  not that the text is similar.

So what does this mean for us, right? We're not actually controlling what a successful match is defined as. And to be aware of the significant gap between the text embeddings and the rankers you use will ultimately determine how we define a successful match. Maybe you're doing internal documents or maybe you're doing, healthcare or legal or tax.

Those things aren't immediately obvious. And you can imagine a situation where embedding models can't really figure out the difference between a neither or nor. And now we can put them into the context of a language model  and it could confuse the language model.

Importance of Logging Relevancy Data

And so what's important for you is to actually start thinking about logging all the relevancy data, right? We've done this with a synthetic data, but logging the relevancy data in a way that you can use a language model or humans to model relevancy directly. And then think about training and fine tuning anybody model or re ranker.

I've seen a lot of companies hire a machine learning engineer to do this, only to realize they didn't start logging right away. And now they have to wait three or six months to get all that data. And so my suggestion or my recommendation for anyone who's hiring is to start collecting this data now. And once you get to a point where you're confident, you can hire someone or bring someone in to help train these separate models.

So what does this mean for you right now? It means that if you start logging today, you can start collecting the relevancy data you need to improve the re ranker. Maybe you can save the top 20 or 40 different chunks and use a language model to mark the relevancy. And you can align this prompt to make sure that it's actually doing what you want.

Then you can train an embedding model or train a re ranker and make sure that, something that was relevant in the 20th position before training is can end up in the fifth position afterwards. And now it can be a lot more context efficient, or understand what relevancy means, and this can have many implications downstream of your product.

 Now let's talk a little bit  about the world of what representations really mean, and how we can think about using synthetic data again to improve some of these metrics. The key takeaway I want you to have here again, I'm going to repeat this over and over again, is that we're basically assuming that the question embedding and the object embedding or the text embedding is similar for some definition.

And that definition is probably pretty weak, right?  For two bios in a dating app, are they similar? Doesn't really matter. Do they like each other? Given two songs, are they similar? Is that really what matters? Maybe we're at, maybe we're building a, add more songs to this playlist feature then it matters.

But if we're building the Spotify discovery weekly. is going to be a different kind of data set. And we can have better models. Obviously, we can use things like BGE or modern BERT, but again, the data is the thing that matters more.

We've even been shown at the team at Sentence Transformers, for example, has even shown that even with just 6, 000 examples, we will ultimately be able to do six or 10 percent better just by having 6, 000 examples. You can fine tune these embedding models for about 40 minutes on your laptop and you can get a lot of lifetime value just with something so simple.

Using Synthetic Data for Fine-Tuning

 Just like what we talked about in session one, if you have no data, we can start thinking about synthetic data. 

And this isn't just thinking about things like having the embedding of a query match the embedding of a text chunk. For all the tasks where we evaluate the success of the search system using recall, embeddings can now be used to  fine tune the model. We could think about making sure that the embedding of a query looks similar to the embedding of a summary of an image, or even the image itself.

We could make sure that the queries embed closely to the code snippets, tables, table chunks, tool descriptions. All of those things matter.

And as long as we do that, and as we log it in production, we can use the data set to fine tune better retrieval systems. We can use large language models to add better relevancy tags, and that will allow us to improve basically every part of the system that we talk about in the next six weeks.

In session four, we're going to talk about things like using clustering and topic modeling to do data analysis. And if you've already trained an embedding model on your data,  the clustering models you perform and the clustering you perform using your data will also be  higher quality.

And honestly, everything that we're doing today with language models is what I used to have to pay data labeling teams hundreds of thousands of dollars to do every year. This is effectively what the machine learning playbook used to be like, but it was only accessible at large companies who could pay hundreds of thousands of dollars.

And now what used to be completely inaccessible for small teams can be done just with a couple of prompts with a policy and a for loop. And so I really wanted to think of seizing the opportunity and getting these very big wins in recall, six, 10, 20 percent improvements in recall, but just spending probably a couple hundred dollars of API calls.

And I really can't stress this enough that you should really try to seize this opportunity to create synthetic data to improve your evals. A long time ago, you would have to build a product with no AI just to collect data. Then we would take that data, then use that to train a model in order to release a product with

Now we get a product for free, but many of us are forgetting to start, but many of us are forgetting to collect data in order to improve the systems that we had before. So it's pretty hard to go from 99 percent to 99. 9%, but many of the systems I'm looking at today are at  70%. And the effort it takes to go from 70 to 85 to 90 isn't that big?

It might only cost a couple thousand dollars of API calls. Before it would cost, tens of thousands of data labeling efforts.

But without having to train humans, LLMs allows us to create this label dataset backwards.  This is a game changer.

And this practice of using synthetic data to create evals,  to create fine tuning datasets, to create few chart examples. This is the wax on wax  📍  📍 off moment. We're going to take the synthetic data, we're going to work on logging, and when we have 20 examples, they become evals. If we have 30 examples, they become few shots.

And if we have a thousand examples, now we can start fine tuning.   📍  📍 And this pattern is going to just happen over and over again.  📍  📍 It's never going to be done, it's just going to be better.  And once you get those thousands of examples, you can start training better models, think about a better product, and ultimately try to make sure that your customers are happy with the results that  📍  📍 you deliver.

 Fine-Tuning Techniques and Examples 

So now let's talk a little bit more about what fine tuning actually is about. But let's think about the re rankers and the fine tuning.

So ideally when we're fine tuning examples, we can create these things  📍  📍 called triplets. And what we're trying to learn is that for the negative examples, the things that are not relevant, we're learning to push them apart. And when we know something is relevant, we're trying to bring things together.

So what happens when we start fine tuning? So if you look at the image on the left, what is before fine tuning, there are some examples that are negative or irrelevant, and there might be closer to the anchor. In this  📍  📍 case, this would be like the embedding of the query vector.

And then we might have data that is relevant, but might be farther away. This is the thing that's on position 20, and after we fine tune, what we can try to get them all to do is move relevant examples closer to the vector and move negative examples further away from the vector. And if you want  📍  📍 a good case study you can take a look at the engineering article that Ramp came out with.

And again, this could be used for tool selection, this could be used for retrieval. This could be used for a ton of different ideas. If you have data in the beginning, you can start training a system in such a way that the similar things truly are similar for what you care about.

So now I've sold you, hopefully, on this idea that maybe we should consider fine tuning. And with enough examples, your model will ultimately learn an objective that's custom to you, that has captured some of the relevance information of your customers, and maybe some of the relevance information that is bootstrapped by a smarter language model.

 If you want pairs, 📍  📍  you just have to make sure you have a question and an answer. And if you want triplets, you need to have a question, positive examples of relevant documents and negative examples of irrelevant documents. And so for example, you could say,  the positive examples are the texts that we included in the context and we cited.

And the negative examples might be documents in the context that we didn't cite. This would be a very aggressive form of defining some of these things.

And what you might find is if you have high quality data, you can probably use this information to train both your by encoder model. So this would be closer to the open AI embeddings, but you can also train things like cross encoders. So this would be more like the cohere re rankers. And if you have the bandwidth and the appetite, you can also train even more complex models, like late interaction models and the Colbert models.

But generally you need both. You'll always need the bi encoder models and you'll always need the cross encoder models.

And again, with a lot of these cases, the data that you generate from your product will be the most. It is not the model itself. And so your job is just to try the same data set across multiple models that you find on the leaderboards and just figure out which one has the best for your use case. And what I'll probably say right now is that Cohere's re ranking API is probably the best that you can use right now.

It is very easy to fine tune and serve data sets. These embedding models and these cross encoder models and these re rankers on your data set at any scale. And if you want to understand how much data you need to start fine tuning, I personally believe that you only need a couple thousand examples.

So maybe one or two thousand is pretty good. And if you have six or ten thousand, you're probably going to get a ten percent improvement in recall just right off the bat.

And what I'll say is, if you already have a lot of data embedded, It might not be worth fine tuning an embedding model because you would have to reapply new embeddings on your vector database.  In which case, I would just start thinking about retrieving more chunks and then passing them into something like Cohere's re ranker models.

And this kind of data is effectively the keystone to contrastive learning, right? This idea that we have both positive and negative examples for every piece of relevant data.

And I'll let you pause here to take a look at what the negative examples look like and what the positive examples look like.

And when you're thinking about fine tuning,  one of the things I really want you to think about is whether or not this kind of definition for similarity is in the training data. Was OpenAI trained on things like Amazon reviews on what is a similar comment? Probably. But we really have to ask ourselves does OpenAI have data, for example, on what you're building?

And maybe the answer is yes, maybe the answer is no. But you're going to find out once you start fine tuning and seeing  how much better some of these models are once you use your own personal data.

Now if you go back to the coffee example before, we can have an explicit way of specifying whether or not they're similar for our use case.

We can push things that are closer together and push things that don't go together further apart.  And we can just do that by having those three example vectors.

The fine tuning system will effectively bring the complementary things closer together.

For example, we could define that I love coffee and I love tea are similar because they're positive. And that they're dissimilar to something like I hate tea. Whereas, we could also say I love tea and I hate tea are the same because it's actually around the same preference, whereas I like coffee is very different.

But now we can try to figure out what kind of external feedback we can get from our users to create these pairs. And in office hours, feel free to ask me about your use case and figure out whether or not this makes sense. I really like to think about how we think about Citation, for example, or processing tables and relevance.

This is just a very small toy example that we can think about.

Resources and Recommendations

I also can't recommend these resources enough. So the first one I want to share with you is just the sentence transformers library. 

And they support both pairwise data sets.  As well as triplets, which is the type of data set where we say this is similar to this, but not similar to some other thing. And I'll just add that having these negative examples is incredibly valuable. Again, if you think about the example where we have the I love T, I hate T thing.

This is a much newer resource. This is something I have not done too much experimentation with, but if you actually look at the sentence transformers library, you'll see that a lot of the models are based off a older version of the BERT architecture.

And what HuggingFace and AnswerAI  worked on in the end of 2024 was introducing modern BERT. So instead of having a 512 sequence These models now have 8, 000 token sequence lengths, allowing us to embed larger data sets and generally just perform a lot better.

And last but not least, I really recommend the Cohere re ranking models. They perform incredibly well. They have high quality evaluations and high quality data sets that they use to make their re rankers.  And they have a fine tuning API that allows you to fine tune re ranking models. 

So Cohere is nice enough to give us a bunch of credits for us to try out. And so I really recommend exploring, playing around and seeing how much better your results can be by using something like fine tuning. And this is the easiest way to get started.

And not only is this the easiest way to get started, this is the, I have yet to see an application where adding a re ranker has not been worth it for the additional latency, but. Usually I see about 10 percent improvement in recall for maybe a 300 to 500 millisecond.  Additional bump in latency. And again, I just want to reiterate this many times.

You don't need that much data to get started. If you look at these fine tune examples,  the dotted line is the performance of the open AI embedding models. And that for something like MP net base V2, even a hundred examples performance better for some of the bigger models, like BGE base 1.

5, even at about 500 examples, you start performing better and at a hundred thousand examples, a lot better in this case, you'll see that for this model is much easier to train, whereas the Gina embeddings was much harder to train. And we couldn't get a model that actually outperformed open AI.

What I also want to add here is that we don't need to train these models to be too task specific. Okay. So imagine a world where we have embeddings for questions and table summaries, questions for text chunks, and questions for, document summaries. I generally don't recommend training multiple models.

Instead, these can all be blended into a single training data set to train a single model, and we can use this to eventually replace the OpenAI embeddings. And in many cases, training multiple tasks of similarity at the same time can lead to better results. So if we go back to the e commerce example, maybe we want a mixture of, two things are together when they're in the same cart and a mixture of two things are together if they're on the same receipt or owned by the same people.

You can make a mixture of this data set, have evals for these specific tasks, but really  the idea is that if you just start collecting data now. you will at some point produce models that are much better than open AI.

And before we wrap up, I really just want to share some examples of success stories and case studies from other researchers,  right? We've seen a 14 percent accuracy boost over baseline retrieval just by fine tuning cross encoders.  We can find a 20, we can find a 12 percent increase in exact match by mapping to,

by training better passage encoders. So these would be by encoders.  And for a lot of companies, we can find a 20 percent improvement in response accuracy, right? Just by using re rankers and even a 30 percent reduction in irrelevant documents. And irrelevant documents will also make your answers worse depending on what kind of model you use.

And if you want to learn more about how we can fine tune models or embed models or host models take a look at some of our articles. In this one, we embedded all of Wikipedia in 15 minutes.  And this is just because modal has allowed us to do very massive paralyzation.  This way, if I have 10 or 15 different embedding models and I want to know which one works best, instead of running it for nine hours, each one could take 15 minutes and the iteration speed of massive paralyzation for embeddings will allow you to have very short turnaround times.

And so drastically lower the cost to run any one experiment.

And this paralyzation also works for training models. When you train models, there's going to be a ton of different parameters. What's the learning rate? What's, what does this variable look like? What does that variable look like? with something like model labs, you can effectively just say, I want to allocate 50 GPUs for 20 minutes, and I want to train 200 different models.

I'll pick the one that works the best. And that's how a lot of model training is done. And so if you want to experiment with fine tuning models. On GPUs, I would say Modo labs is a great place to start as well.

This is a pretty short video and let's just talk a little bit more about what I really wanted to think about this session. And we'll give a preview for the next session as well at the end.



Homework and Next Steps

Before we wrap up, let's just talk a little bit about what the homework  📍  📍 looks like for this week, for this session, and go over what we'll talk about next session.

The goal is to keep sessions one and two pretty lightweight as people start onboarding and getting set up. But again, last session, hopefully you were able to create some synthetic data to test your system and establish a baseline to run experiments on, try things like BM25, try things like embeddings, chunkers, and re rankers. With these baselines, you can actually convince yourself to do nothing.

If experiments don't pan out. And if you can sample some percentage of real user traffic and run those LLM re rankers over that retrieve text chunks, we can figure out what has low precision. The idea is that you want to start thinking about and building the data flywheel.

For this session, the focus is on improving your representations. , for those same subtasks, if you only have 20 examples or 40 examples, you know, you don't have to worry about it too much. But as you start getting thousands of examples or thousands of samples from your production datasets, now you can start thinking about having baseline recall metrics for different chunks and different snippets and et cetera.

And we can ask ourselves, how can we prepare a triplet data set to potentially train a cohere re ranking  📍  📍 model, even 500 examples let's just jump into the next session.

The goal for the first three sessions is really to understand How we can set up the fast evaluation cycle Use precision, recall, really focus on retrieval.  This session, the idea is that , not only can you focus on retrieval, as you create more data, you can use that same data set to improve retrieval.

The next session, we're really going to go into thinking about how we can build a UX that feels fast, but more importantly, helps us collect more data in the future.  This will be the first flywheel that we build before jumping into the more advanced topics for the rest of the course.

And as always,  if you have anything that's particular to your own data sets, feel free to DM me on Slack. See you next time.