 

Introduction to RAG UX

Welcome to session three of systematically improving RAG applications.  In this session, we're going to talk about the art of RAG UX, and in particular, how we can turn design into data. 

The three goals for today is to make sure that we're taking actions to collect feedback. Expand on what is possible with streaming and give a small set of prompts and UX tips to  improve satisfaction and quality of the answers you generate. Most of this will be a survey of other techniques that I can apply here, and this does not  necessarily fit into the other sections of the course, but the general idea is we want to collect as much feedback as possible.

And you'll really be surprised at how some minor changes can have, two to five to even 10X more impact on our ability to collect high quality feedback.

The first two sessions have been around faking the data that we generate in order to create synthetic data in hopes that one day user data will allow us to supplement the work that we're doing with synthetic data. In this session, the goal is to figure out how we can collect that user data and how we can give the users a good experience.

Collecting User Feedback

So in the first two sessions, we talked about the importance of looking at your data, and in this session, we're gonna talk about how we collect feedback.

User feedback is the second most important thing after looking at your input data. And in particular, how we can think about collecting that feedback and how we can drive the users to give us that feedback. So I want to drive a simple example. Let's take a moment to look at this design.

What kind of behavior do you think this is encouraging? And the question to ask is, does this behavior help my team in terms of collecting data? Now look at the second one. What kind of behavior does this model encourage? Does this behavior help my team collect the data we care about? It's a couple of small differences. Where can I give the feedback?

Is it hidden? How long did it take me to find the feedback buttons? Does the copy change? These are all super important. 

Designing Effective Feedback Systems

And we will notice is in the first example, we have very small  buttons. And in the second example, not only are the buttons  massive, we also think about the copy that we collect, right?

This is actually really important. Sometimes when we have negative feedback over some of the answers, It's not really clear what exactly is going wrong.  It could take too long. It could be too wordy. It's very unclear. But if we actually changed a copy to something more specific, did we answer your question today?

Did we successfully complete the task? This copy will allow us to align the outcomes of what we're trying to measure. And these labels will be very useful. Once we start looking  📍  📍 at. 

Using Feedback for Data Analysis

sessions four and five, we care about doing data analysis over what kind of questions we underperform in.

And then what happens is, when you select something like negative feedback, when you say, no, this didn't answer the question. Now we can start asking her follow ups. Why wasn't your question answered? Was it too slow? Did it call too many functions? Is the response format bad? Is it misinterpreting results?

I've had so many clients in the past, not really understand what went wrong in the system. and collect feedback in a way that is very uncorrelated with any kind of these outcomes. And if you look at the feedback that's positive or negative,  📍  📍 they don't even correlate with evals? Sometimes people think it's bad because it's slow.

Some people think it's bad because it's too long or too short. And now these are all different things that we care about. And these are going  📍  📍 to hurt our ability to do data analysis. But by having some of these sub questions, there's going to be certain questions that we can segment on. To help us understand what the failure modes are.

 What we mentioned before likely will work for a consumer setting? Very  📍  📍 low touch, small little feedback buttons in order to help us understand what's going on. But this also works in a sales setting, especially when we're working with very large customers and we  📍  📍 have something like a customer success team.

Behavioral Insights and Customer Interaction

It turns out there, a lot of it is actually going to be behavioral, you really have to let the customer know that the negative feedback  📍  📍 that they give will  improve the product or be of use. And so a lot of the time we think about things like having different Slack bot integrations and having webhooks where negative feedback is directly posted into Slack and then we can review them with  customer  📍  📍 success folks 

then we can take those examples, review them by hand, add them to our eval suite, and  📍  📍 then bring it up next session, whether it's some kind of bi weekly or monthly meeting with the team. We'll bring up the negative feedback, we'll tell them that we've added them to our evals and then report back on how much we've been able to improve over time.

This has been the biggest factor in allowing us to collect feedback for customers while building trust, collecting data, and building our evals. In both cases, in the case where we changed  📍  📍 the copy for consumer feedback and the cases where we created these shared slack groups, we saw about five times more feedback for the same applications,

And what this means is we can fine tune five times faster or having evals and be able to deploy something knowing that our models works five times faster.

And like we said in session two, this same feedback can be used to improve our re rankers or embedding models. For example, on cases where people not only say we did not answer the question, but the data was irrelevant, we can now go back, use a language model and relabel some of the text chunks. We can relabel them for things like relevance in order to figure out if we can find any hard negatives to mine, or whether we can create synthetic data in a certain region Where our customers are really unhappy

especially when it comes to things like negative examples or having feedback that says that the language model is confused, really what we can do is we can potentially have different UX that allows us to label hard negatives. And so if  recall our fine tuning session  and think about this idea of triplets, 

we generally have the idea that.

Given some kind of anchor, we know what is a negative and we know what is a positive, and after we fine tune, we can move the negative examples further away and the positive examples closer together.

And the hardest part of this task is actually how we can find these hard negative examples.

So I want to present a different piece of UI. If you think about, Facebook's people you may know feature, we could have built a UI that allowed us to have some kind of infinite scroll as we built this out.

And here we just have a bunch of different users and we give them the ability to add something, what happens is every time you add somebody, that could allow us to  say, you know what, if we clicked user 11, maybe that should have showed up higher in the front and we can build some examples with positives.

We can also think about how we can build things with negatives.

Another way of building the same UX that's not using infinite scroll is to provide a limited set of profiles. Here we might have a version where we only show the top five profiles. And when you add someone, it disappears and we show you another. And if you delete one, that's also a way of collecting more feedback.

By deleting one of these examples  📍  📍 in order to see another one , what we're really doing is we're allowing users to label hard negative examples. And now we know that these are irrelevant. 

You can imagine a UI where not only do we answer the question, but we show you the documents that we reference. And if a user can delete one of these documents and regenerate an answer, that's another opportunity to think about hard negatives.

And there are numerous ways to collect positive and negative examples.   📍  📍 If you think about the ,  Hinge or the Tinders of the world, they train a  embedding models because one, they have a ton of volume in terms of their interactions,?

 📍  📍 If you think about the swiping mechanics, but also, we not only have positive examples like likes and dislikes, we also have very simple objectives, like whether something is matching. And that's the general secret of how we can collect high quality data. Positive examples, negative examples, and simple objectives.

And again, like I said before, this is something that we can do with citations, which will allow us to collect more feedback and build more trust within the system.

 For example, 📍  📍  if you build a system that allows citations,  📍  📍 we could cite references within the text . Citations can go a long way, especially if your end users or customers are not familiar with AI. They're going to  ask questions like, where do we get these answers from? Where is it coming from? How is the AI able to get this information? And how do I know it's accurate?

Ultimately, most of  these questions are going to be around building trust, and  understanding of what's happening behind the scenes. And you can beat them to the punch by including things like citations that they can interact with.

Not only that, citation can allow you to have a preview of what's going on, right? What data is being used. And by having features like being able to delete citations and regenerate answers, we're going to be able to collect that negative feedback.

And if you currently don't use citations  📍  📍 today , this is a simple way of doing so. You can define some kind of tasks that can use the markdown templates links to create citations.  📍  📍 and then you just have to include the text chunks with some kind of index.

 📍  📍 

Another way of doing this is having your LLM output structure data.  📍  📍 Here,  I'm outputting a regular body with citations in the form of markdown links, except now I actually am able to create titles, for example, that mapped to different chunk IDs that I use to cite these answers.

There's a lot of different ways of being very creative, how we can prompt these things. But generally, many of the APIs these days are actually coming out with their own citations API, for example, for Anthropic and for Cohere models.

What's been more interesting the past couple of months is that not only can you cite the text and the text chunks  that come from the documents that you extract, but now we're entering a world where we can cite also the bounding boxes of data that we parse. This way, if we have an answer around the titles or some tables, we can cite the actual box.

And this would allow us to not only answer questions with a high level of fidelity, but also to have citations be visualized over  the actual PDF, rather than just the processed text.

The Power of Streaming

The second thing I really want to talk about today is the 

importance of having better streaming. Only about 20 percent of the companies I work with have 📍  📍  a good understanding of how to implement streaming in order to make  📍  📍 the user experience  better.

 My recommendation is that. When possible, try to stream everything. You can stream interstitials to explain  📍  📍 the latency and help customers understand what's going on. You can stream different results and different UI  📍  📍 components so customers don't have to wait for the last token to complete. 

 And lastly, you can stream tool calls to show 📍  📍  intermediate states  you see them, it's really hard to  unsee. Whether it's Slack or Facebook, or LinkedIn, skeleton screens really help users improve their perception of latency.

 And the reason I really recommend this is that streaming has become table stakes in many LLM applications. Your users expect responses instantly and by streaming you really improve the performance and the perceived performance of the system. 

 For example, users perceive animated progress bars as 11 percent faster, even when the wait times are the same. 

And for most applications, users will only tolerate up to about, up to about eight seconds  📍  📍 of waiting given visual feedback , ,  📍  📍 which reduces abandonment.

And lastly, streaming improves satisfaction and trust. 

Applications with engaging loading screens often report higher satisfaction scores. And Facebook, for example, discovered that skeleton screens really reduced the perceived load times, resulting in better user retention and engagement.

We could stream the responses, which could include citations or follow up questions. We can stream the arguments of different function calls and even interstitials to show different states, especially as we're doing more retrieval.

If you're on the fence about implementing streaming in the products that you're building today, I just want to let you know that migrating from a non streaming application to a streaming application is a pain in the ass. And I really recommend building it from the start, it's likely going to take weeks out of your dev cycle just to upgrade to streaming in the long run.



So for example, I'm going to generate an answer here that uses streaming to, to load out the answer.

Here you see the answer being generated. with citations and follow up answers.

Huge improvement.

 On top of that, if you use a library like Instructure, you can also stream out structured data.  This is what allows us to actually have the stream results. Here, I say the citation has an ID and a title. And then my response to a user contains both the content, my response, follow up actions, and then sources.

And now I just have to build a React component that renders all of this. And you can see, the data will stream out, the citations are rendered, and then as well as that, we see that follow up questions are being generated. Then when I mouse over different components, we can figure out that, okay, this is a citation that references this text, and that is a citation that references that text .

 And this idea of building up follow up actions in the UI is  very powerful because now you  📍  📍 can use this data to track and figure out how users tend to follow up and then include that in the few shot examples.

And if you want  📍  📍 to see an example of how this might feel in practice, I have a claude artifact here that shows you the different loading  📍  📍 times . And as you can see, the text on the streaming input already is starting to come out. with interstitials, whereas the non streaming panel still is an empty screen.

And then as this one completes,  the text will just flash  into the panel. And again, to have these interstitials as the ones you see here, where we had different loading bars, it's very easy to use something like server side events to, build something like this.

 I also want to give a small example of what this might look like as a Slack bot.

Especially when it comes to interstitials.

For example, for a Slack bot integration, you can just react with the eyes emoji to communicate that the user message has been received by the Slack bot. It doesn't have to be that complicated, but just let the user know that, okay, this has been received. And then we could do, for example, is market with a checkmark emoji to label it as having been responded to.

The difference between just responding to the text message is one thing, but you could also do something like this. You could prefill the emojis with thumbs up, thumbs down on the star reaction  to prompt the user to think about giving feedback. 

If these buttons were not there, if these reactions were not there. We might not think about thumbing up or thumbing down these situations. But because we ask, how did I do, and provide the thumbs up, thumbs down, and the star, now we're  giving the user, an opportunity to give us feedback, ?

You can imagine having this is gonna dramatically improve the amount of feedback we get compared to just having  📍  📍 no react emojis at all.

And as always, we can save these question answer pairs as few shot examples, or even cache them in the future.

And this is generally the principle I really want you to think about this week. What are all the simple ways that we can communicate our latency, stream data out, and show these interstitials in order to give the customer and give the user a great user experience.

Prompting and Chain of Thought

So now let's go over prompting these language models and specifically thinking about things like chain of thought.

I also I just want to call out the fact that not all prompting should be for the and we should also prompt the customer and prompt the user.  Generally because people are pretty lazy and most people don't know what they want. 

And so by just even giving a couple of examples early on, it's going to make life as easy as possible. This is also a good way of using these suggestions as an opportunity to show off something your users wouldn't have thought about or didn't know was even possible. For example, when perplexity had social, I didn't even know that was a thing.

And in session four, we'll talk about how we can discover these through data analysis of your existing conversations.

You can also see in this example with perplexity. We are able to show different sources. We have special UI components that match the different capabilities. But generally, there's a very strong way of tying the sources and the text back to the results.

Here we have follow up queries and related queries. We have share buttons and copy buttons. Again, this could be a way of collecting feedback, a way of letting us know that the generation was good, and there's a real opportunity to use this white space to create different blocks in your UI to really showcase your capabilities.

And I just want to call out, as you're trying the next couple of AI tools, really pay attention to  📍  📍 how they are collecting feedback, how we're rendering citations, and how do we continue the user experience by adding these follow up actions.

And what you're going to notice is once you start looking for these interactions to help collect data, you'll see them everywhere.

In session four, we're really going to cover how we can figure out what kind of capabilities our language models are able to do and which ones  📍  📍 we were not able to do. And one of the things you're going to realize is oftentimes you can make your application much more reliable just by rejecting work.

Monologues and Chain of Thought

 📍  📍 Now , I want to talk about monologues and chain of thought. This is a massively missed opportunity for a lot of teams. But I think with the advent of things like O1 and R1, we already know that this is a game changer in terms of our ability to improve performance. And for a lot of the implementations that I've seen, even without R1 or O1, implementing chain of thought in the ways that matter to your business has been one of the highest impact things that we could do.

In many tasks that we've seen so far, chain of thought is often able to produce a 10 percent bump in performance, and that bump in performance can make the difference between something that is usable and something that is impossible to deploy in production.

And just by wrapping chain of thought in either XML or streaming it in with something like O1 or R1, we can then again build dynamic UI that renders the chain of thought as separate data. And we're already seeing this with, the ChatGBT and DeepSeq models. And as a result, chain of thought can now also become a loading interstitial.

And for things like monologues and chain of thought, we can leverage them for multiple purposes. For example, when dealing with very long contexts, the language model may struggle with things like recall or fully processing instructions. And having the chain of thought reiterate those instructions, and recall all the relevant examples before answering the question, Again, we're able to reorganize the context in a way that allows the language model to re read the prompt, improve the reasoning, and ultimately get a much better result.

And I want to give a specific example of using monologues for something like creating quotes for SaaS pricing. This is a small little case study. To set the scene, imagine a situation where we have a 15 page pricing document. on how we want to recommend salespeople dictate prices for a customer.

And then imagine we have a one hour long transcript.  And the task is to take the pricing document and the transcript to prepare maybe an email to figure out how we can propose the pricing model.

Before long context, we might have to do some kind of rag to figure this out. But what we found in practice is actually much more effective to just cache prompt, for example, the pricing data, and inject a single call transcript. And instead of using things like agents, we can just very specifically prompt the chain of thought to behave a certain way.

In this prompt, we ask the language model to first reiterate the variables that define the quotes that we care about. Then we reiterate the parts of the transcript that mentions the quotes. And then lastly, reiterate the parts of the pricing document that are relevant. And what this has done is we've basically had the LLM naturally bring all the relevant information into the chain of thought.

And then when we ask the language model to reason what the pricing options are, with a single prompt, we were able to get our pricing question answered without any kind of complex multistage agents.

This allows us to make sure that our follow ups are very high quality and we were able to produce citations that are very  easy to verify. And as our sales rep to verify the generated quotes, we were able to use this to create more training data, more few shot examples, and ultimately build a better product.

And we were able to get to a place where 90 percent of the follow up emails were accepted without any edits.

Generating monologues can also dramatically improve things like tonality and quality, which again can be fine tuned later to be done without monologues.,  so the idea here is we can use chain of thought to create a better answer and then distill that into  📍  📍 a smaller model.

And here I'll let you pause to really read the prompt, but these are some examples of how we might want to use chain of thought without these reasoning models. So this would be using something like 4o mini or using a sonnet, for example.

And here's the example of what that looks like for the monologues.

And now that we talked about using something like chain of thought to improve generation, I wanted to talk about one last thing, which is using validations. 

Validation and Error Handling

Oftentimes a single step is not enough to have high quality answers.  📍  📍 And I like to have a validation pattern before going into any kind of multi stage agent behavior.

It's my opinion that as our language models get more and more complex, we're going to be able to do more and more within a single prompt. And what might take an agent today will likely be possible with a single prompt and a couple of function calls.

And so the last level of product change I wanted to really think about. are  📍  📍 validators. In latency insensitive applications, having a validator in the outputs of your prompt can really increase the user's trust and satisfaction of your product. And these effectively become evals and tests within the production workflows.

For example, we can use an external system to evaluate whether the reasoning, whether citations, or the generated response effectively answer the question. And if they don't, the system can provide detailed feedback on what's incorrect, What needs revision, and allow the language model to try again.

A very specific example that mattered for our use case was to include URLs that were completely valid. 

 And so in this example, we wanted our language model to draft emails with references to case studies and marketing material, but we had to make sure that we were not sending any hallucinated URLs. So when we  📍  📍 built our  structured output model, which had a subject for the email and the body, we built a simple  📍  📍 validator that just used a regular expression to verify that the URLs existed in the allowed URLs.

In this case, it would just be our own company website.

 By building this validator that uses the regular expression, we can really easily verify that all the URLs are allowed. They're in the allowed domains. And for very critical applications, we found that  we can even make a small get request 📍  📍  to each URL just to verify that 200 status.

And what happens is if there are any issues, we will just let the language model know and request to either regenerate it without the response,  Or find the correct URL. And what we noticed in this application was that initially we had a 4 percent failure rate. So 4 percent of all emails had a URL that was invalid.

Either it was not from the allowed domain or there was some kind of 404 error because it didn't exist.  After one retry, it got to 0%, which means every URL was correct, right? This is all, this is easy because we just simply removed it. But this was using the 4. 0 model. And after fine tuning the 4. 0 distilled model into 4.

0 mini, we noticed that the validators never  📍  📍 triggered again. We got into a place where, even without the validators, we would get a zero hallucination rate in a single pass of the model. This took about three days to build, and it allowed us to have a much faster application because we weren't having to do the retry loop.

And like I said before, even in February 2025, deep research will still include fake links when given the opportunity.

Conclusion and Final Thoughts

So this concludes the first half of the systematically improving RAG course. And in particular, it covers you don't have any information,  we can understand that this feedback is being used to improve our re ranking models. 

And now in this session, we're really thinking about how we can use different UX to one, not only make the user experience better as the customer is using it, but also build a system that allows us to collect feedback over time, whether it is deleting citation to regenerate answers, whether it is changing the copy so the customer understands what they're actually labeling, to even adding validators that can check in code whether the answer is correct.

And now the questions  I want you to ask yourself in this session, whether it's for your work projects or your personal project is, is the following.  One, are you being too subtle with the feedback that you're collecting? Should we change the copy? Should we make the buttons bigger?

And how can we communicate to the customer that their feedback is, very important? 

The second thing would be, am I building citations in a way that will help me gain user trust and improve satisfaction? And can I use the citations UX to collect more relevant data? 

Third, Could I be implementing streaming better and loading interstitials and follow up actions to make my applications feel faster? 

And can I better promote capabilities and reject work that my language model can't do?

And lastly, can I iterate on things like producing monologues and chain of thought to reiterate parts of my data and parts of the context in order to improve the reasoning in my system? 

 And as always if you have any questions, feel free to ask in the Slack channel and we'll see you next time