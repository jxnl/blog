‚Ää

Introduction to Session Four

Welcome to session four of Systematically Improving Your RAG Applications.  üìç  üìç This is probably my favorite session. And the general ideas of this session are going to be around how do we segment the data that we have coming in from our users and from our customers, and understanding when to double down and when to fold.

 

Recap of RAG Flywheel

Let's recap the RAG flywheel. 

 So far we've talked about the initial implementation of having a  üìç  üìç RAG system in place.

Then the goal is to create synthetic questions to test the system's ability to do retrieval. And here  üìç  üìç we're going to evaluate things like precision and recall, and just generally how we can improve our ranking. 

As we blend in user  interactions, we make sure we can collect user feedback by building better UI, as we've talked about in session three, and ultimately thinking about things like fine tuning either the embedding model or the re ranker using the data that we have from our own system.

This gets you to a place where you have a reasonable product to deploy. 

Post-Production Data Analysis

And now we're going to talk about what we do post production. What do we do when we have  üìç  üìç plenty of data coming in ? And  üìç  üìç how we want to think of doing data analysis to figure out what we should do next.

This is the actual playbook, and this is the thing I'm most excited to talk about. We're going to focus on segmentation and analysis, we're going to figure out what's missing, and we're going to figure out where our blind spots are and what we need to improve.

Then, once we figure out these specific segments, We can think about how we navigate multimodal RAG and focus on system improvements for each specific segment  and  these segments might be addressed by building specific tools

 Then once we have a series of tools  üìç  üìç to use  we can blend a sequence of function calling steps into a single system, bring it all together, solve this query routing problem, and make sure we have the right retriever for the job.

Importance of Segmentation

So first let's look at why segmentation is important and give an example in the marketing context. 

Segmentation in Marketing Context

I think the marketing context is a very simple one because we've been doing this for a very long time. You can imagine that you're selling a consumer product and you run a marketing campaign to boost sales.

And as a result of your efforts, maybe there's an 80 percent boost in sales, but you don't really know why. Alternatively, you could also have an 80 percent drop in sales, and you also wouldn't know why. But if you were a data scientist and you were an analyst, what you would do is you would rummage through your sales data, look across different customer segments and realize potentially that maybe 60 percent of the sales increase or decrease might be coming from a specific segment.

Maybe 30 to 45 year old women living in the Midwest. This is the case at Stitch Fix If we were to identify this what we could do then is use this information and decide Is this in an audience that we want to invest more efforts into? How could we target this audience better? It might not be the case that we want to be running Super Bowl ads, but maybe there's something else that we could figure out.

Maybe there are podcasts where the demographic, for example, covers the demographic we talked about.

And when we identify these segments, we can also identify them in the way that we want to. 

Applying Segmentation to RAG Applications

Our customers use our RAG applications. And ultimately, it's our responsibility to uncover this and target this.

If you can properly identify the demographics and psychographics of the inputs we might be able to identify multiple levers to experiment with, but also, more importantly, allocate resources in a very effective way. And then you can start to explore and exploit the system.

Right here is a graph of what sales volume might look like. Maybe 10 percent of our customer base is actually leading to 60 percent of our sales volume. But now we know that maybe the goal for next quarter is to invest more in Segment 1 or grow Segment 1 or understand why Segment 1 is outperforming.

Or we might notice that 40 percent of our customer base might be resulting in 10 percent of the sales. Maybe something's going on. Maybe we're not marketing to them effectively. Maybe we should consider not onboarding them at all  in order to allocate more of our resources to the better performing segments.

This kind of thinking will be the same for your queries.  There's going to be queries that have amazing performance that you need to target.

There will be queries that have really good performance that you want to highlight and show off to your customers. But there's always going to be queries that need a lot of targeted improvements for you. And one answer might be to make that investment, but another might just be deciding that it's not worth the time.

And in the context of a marketer, we can do very similar segmentations.  We could segment our data based on the role or an organization ID. We could segment our customers based on cohort or life stage. We can even segment our customer base using psychographics, maybe their attitudes or their values or their interests with the writing styles.

Now we're thinking about how we could maybe segment based on the summaries of embeddings or the embeddings of summaries over chat histories, for example, and this is something that Anthropic now does quite well.

And once you start cutting up your data in these ways, you're gonna start noticing and segmenting and you're gonna reveal a bunch of problems within your app that you would never notice before.

For example, maybe I have the, a query, like what's the difference between 2022  versus 2023 budgets. And now I can tag these queries. Maybe it's a query that requires a time filter Maybe it is a query that requires multiple queries. Maybe it is financial. By just having these tags, we can do a lot more interesting things.

For example, I could group by the time query and just count how often it's happening. Or I could figure out whether or not customer satisfaction is different for time filters. Maybe you could figure out that the customer satisfaction is different in aggregate for multiple queries or time filters on something else.

We would learn a lot about where our system is failing.

Segmentation and Decision Making

So now let's get a little bit more specific on how the segmentation supports decision making.

If we have an understanding of segments, then one thing we could talk about is the fact that the expected value of our system is going to be described as by this pretty simple equation. It is the impact of the system you've built, maybe that's just revenue.  It is the percentage of queries within that segment. 

And the probability of success. And if we could add that up across our segments, then we would understand how valuable the system is.

This is fundamentally how you  improve your application. By breaking down your application using this formula, and just figure out what is the lever that is the most important to tune. 

Identifying Key Segments

Now, it's no longer just making the AI better, but identifying certain segments that matter to you. And here, the probability of success could be  the generation quality, it could be whether or not we have citations, it could be the relevance of text chunks, or maybe it's correlated with something like user upvotes.

But once you've identified your segments, these are the kind of levers we have control over.

Whereas impact, for example, might just be a function of how economically valuable the question is, and the percentage of the queries is generally defined by the kind of UX you have, how you've taught your customers how to use the tool, and generally what your customers are trying to look for. 

The key takeaway here is that if we actually label the query types that our customers ask for, then we can build specialized systems to maximize the impact of every single segment.

We can maximize the probability of success. for every single segment,  and by doing better education and tuning the UI, we can potentially control the percentage of queries for a specific segment.

 And if you're asking, how do we actually do this in practice? Generally, the idea will be to leverage things like clustering models and few shot classifiers to cluster the initial set of queries and the initial set of conversations you have with your and then build a sequence of prompts that can classify and segment these queries in a batch offline setting or in an online setting. And by doing that we can monitor things over time, do historical analyses, and  figure out what percentage of queries exist for each segment, and what's the probability of success.

Now, the real challenge is going to be, how do we estimate impact?  This might be a result of user feedback and user research. It's going to be about understanding how we can measure the likelihood of success.  And lastly, asking ourselves, what kind of product decisions can we make to control the query volume for each and individual segments?

And you see a lot of the work here is effectively collecting that feedback, which is why we spent so much time talking about it in the previous section and why we're gonna spend more time today.

And as a quick review, the simplest thing you can do early on is just having a thumbs up and a thumbs down system. I don't recommend five stars. It's too subjective. Just do something binary. And then make sure, like we said before, make sure the copy is very specific. Don't ask a very generic question like, how did we do, but ask a much more specific question.

Did we answer your question? Did we do the right thing? These labels have to be correlated with the probability of success, right? 

But remember it's important to do both. Better UX to collect better feedback, but it's also very important that you actually do your user research and talk to customers. And if you think about all the different ways of collecting feedback, you can allow yourselves to rate and delete sources.

You can maybe track whether or not the code is being copied, or shared, or published, or even saved. All these things are trying to suggest to you what is a good answer. What is your probability of success?

Once you're able to do this clustering, what you're going to find is, we're going to have two things. Once you're done clustering these conversations and these questions, you're going to identify two key patterns. For each cluster, how big is the cluster? What's the query volume?

And for each cluster, what is the user satisfaction?

This could be as generic as just the average embeddings or the coherent ranker scores, which might be capturing relevance. All the way up to the customer feedback, where someone's pressing the thumbs up or thumbs down. And here's what I want to really stick into your mind. That not all queries are the same.

And I really want you to use this framework to answer the question of what should we prioritize in the future.

By analyzing the query volume and your user satisfaction, you can put yourself in four quadrants.

If the user satisfaction is very high and above average, and the volume is high and above average, generally there's not much to be done. You can just pat yourself on the back and continuously monitor and review these performance metrics for this certain group.

If the total volume for this segment is, very large, maybe you want to consider breaking them down again. But generally, if you're doing very well for a large percentage of your customer base, there's nothing bad about that  maybe the customer satisfaction is very high, but the query volume is very low. Here, we might want to promote the query types that have high user satisfaction and figure out what we need to do to improve query volume. This might mean promoting and highlighting the capabilities to the users by improving the UI.

We could even identify whether or not they're high impact, but just for a smaller customer base. And I'll give examples of that in the future.

And ultimately you can experiment with UX changes to improve the usage and the visibility. For example, you could show some of these kinds of questions below a search bar to tell the customer that, look, you can answer questions like this. And we do quite a good job.

And obviously for some low volume, we might also have low satisfaction. There, it just might be worth doing some kind of cost benefit analysis just to figure out what's going on. There might be a world where we decide, you know what, our app doesn't have to do everything, and I would rather just add something in the system prompt that says we can't answer questions like this or reject the work altogether rather than trying to invest so much effort in a low ROI piece of the question space.

. And obviously, this is the danger zone. This is the most important part, and this is where you need to double down.

If customer satisfaction is very low, but the query volume is very high, now you know what you need to focus on. You might want to conduct more user surveys and focus groups to identify what the pain points are  and set more goals on how we think about experimentation for this group of the customer base.

Generally this danger zone is going to be a group of questions where the customer is really expecting us to work They keep trying to make these queries, but it continues to fail And it is basically an imperative once you have this insight to just go ahead and fix it

and in line with this, I just wanted to share an anecdote that I have from a project management rag project for a construction project.

In this situation, our product team really hypothesized that scheduling was a very important use case for our rag application., But, when we looked at the data, it showed that the users were really looking for questions that used document search. And it turns out that over 50 percent of our queries were document searches, because we built a classifier that just classified over a thousand questions, and generally we had about a 70 percent user satisfaction.

So we might say, okay, we're doing a decent job,

but obviously user satisfaction could be higher.

But soon, when we started plotting these same distributions over time, we found that almost all new users start asking scheduling questions, which have low satisfaction. And as a result, they move towards searching documents that are often related to scheduling. And it turns out that in our situation, high document search satisfaction really masked poor performance in our ability to retrieve the schedules of things.

And so we made a bet to just  improve our schedule search to better understand queries about things like due dates, payment dates, and just understanding whether or not parties, for example, are all signed off And once we were able to address this, our team communicated with our customers that a new schedule switching capability was completed.

The key takeaway here is that there's a bunch of different key areas of improvement that might be obfuscated just by using these simple summary statistics, like the probability of success or a percentage of a user segment. Oftentimes users are also very savvy and might change their behavior based on the capabilities of your product.

Which is why things like user research is very important.

Obviously this is fake data that I asked Claude to create, but this summarizes the general impact. Usually what happens is when we group  üìç  üìç by the tenure of a customer, we find that in the beginning everyone is  making these schedule searches. And they just don't really tend to work. And so as a result, our customers learn to just use document search instead.

And as they are learning that, the customer satisfaction content is now going up. And once we realized that this is what was happening, we made efforts to just improve document search, educate our customers, and we continue to see  user satisfaction improvement.

So as much as I'm talking about things like the probability of success and the percentage volume and the impact and whatnot, I want to make sure you understand that these summary statistics are not enough, and you still need to look at your data line by line.

Not only is it valuable to look at these statistics, but if you do find these clusters, it's very valuable to just go in and make sure That if you see, seven or eight clusters, you might be want, you might want to look at 100 questions per cluster just to understand what's actually going on, you might want to plot that across time or different cohorts or different org IDs to understand whether or not specific customers or specific orgs  might be asking questions in a different way.

Like I said before, RAG is really just a recommendation system squeezed in between two LLMs, and often times we need different retrievers  for different customer bases.

It's not going to be the case that a single global scoring function is going to be enough.

And if you just revert to old school machine learning and data science, the more exploratory data analysis that we do to find these segments that are worth pursuing, the better of a job we will be at prioritizing and using our resources effectively.

We are still going to need domain experts that can do feature engineering and looking at your data and looking at these clusters is effectively the same thing. And once you do this feature engineering, what we're going to really come out of this is going to be. specific kinds of indices that can use additional metadata to improve our ability to do search well in these very high impact segments of our user population.

This is the crux of this entire course. The first three sessions we're going to talk about how to count that data, and the last three is about how do I identify specific indices and specific candidates to improve, how do we spend our resources, how do we improve each one, and then how we recombine them in a complete system.

By now I hope I've convinced you that segmentation is important.  Let's talk about the two specific kinds of segments that we can improve on and what they really mean. And personally, I find that being able to identify and distinguish between these two types is critical. Most of the companies I work with don't really see that there's a difference, but generally there's a very different approach to addressing each one.

Addressing Inventory and Capability Issues

The first is a lack of inventory. This is literally like the inventory of a recommendation system. If there's just limited knowledge in your knowledge base and you can't answer the question, that's an  üìç  üìç issue. And to fix that, you need to extend the corpus of information that you have. You might want to have different data characters and different ingestion systems.

Maybe you want to focus on subsystems that ingest specific topics. Just like in recommendation systems, if you don't have any inventory, if you have low liquidity, you're not going to do that well. The second type of query segment  üìç  üìç that really needs a lot of improvement is the lack of capabilities. This is going to be a lot more complicated. The issue will be fundamentally the functional abilities of the system that you have. It might be the case that metadata might not exist or is not structured very well. And to improve these problems, we need to enhance our abilities to understand things like metadata.  Project dates, or proposal documents, or mapping things to different calendar years. In one example, we had a problem where we had a financial search, but our customers would search calendar year, but the documents were actually in fiscal year. And so there were small nuances in how we were able to do daytime filtering based off those two.

But not only do we need to build new search features based on additional data sources, but we might need to do different kinds of filtering or aggregation.

And again, if you're wondering why this is important, the fundamental reason is that by categorizing these user questions in these segments and identifying whether it is a,  a result of a lack of inventory or lack of capabilities will tell us how we can address these problems, how we can identify gaps in our knowledge, develop specialized subsystems or features  to drive specific use cases.

And by building a portfolio of search functions or tool calls, we can recombine them later on to build a complete user experience for what our customers are trying to do.

Examples of Inventory Issues

Now let's dive deeper into the lack of inventory, right? Some examples of inventory in a traditional recommendation system are going to be pretty straightforward. If you're Amazon and customers are searching batteries or televisions you're not going to return any results because maybe Amazon was only selling books at the time.

But the solution here is probably just go and make sure you stock things like batteries and televisions, and you know it's important because a lot of customers are actually searching for that kind of stuff. Now Amazon has their own battery product.

Maybe you're on Netflix and you have search queries for  üìç  üìç for Spanish television . , if . , you just don't have those kinds of shows, you're going to be out of luck. But here the solutions might be more like improving your subtitles or producing more TV shows in different languages and different demographics.

Again, what you notice  üìç  üìç is that it's missing the inventory . It's not the case that the AI is bad.

And my favorite example is the example with DoorDash, where you might imagine a situation where Greek restaurants near me, have bad search results. What DoorDash ended up actually doing is using this kind of algorithm to figure out, okay, we need to buy  more restaurants iPads so they can start onboarding themselves on the DoorDash.

Again, it's not the case that the AI is bad, just that we are lacking inventory.

So now let's think about what kind of proxies we have for bad inventory. If you have these certain segments, for example, or these certain classifications, if inventory is bad, then chances are we might have low cosine similarities. We might have lexical search return zero results.

Your LLM might not be able to answer questions, and it'll say it can't answer questions due to missing data. It might be the case that your language model is trained to cite sources, but not many sources are being cited.

If something is running in production, it also might be the case that there are data pipelines that are not being updated correctly, or there's broken configurations, or maybe customers are not providing the data they said they would. All these things tend to contribute to inventory issues, and they can be detected in some ways by looking at relevancy scores, or cosine similarities, or ability to cite answers.

  You think about inventory as the number of rows in your database, then capability would be the number of columns. 

Examples of Capability Issues

And if we jump into examples of limited capabilities,

We could imagine a query on Amazon searching for affordable shoes with less than three inch heels. If we don't understand what affordable means for the customer, or we don't have metadata on the heel height, we simply can't filter on those features to give you a good answer.

And the solution might be just making sure that vendors provide more metadata tags.

Or developing features where we can understand a user's subjective nature for what affordable means.

You can also imagine a situation where Netflix has plenty of movies that have been nominated for the Oscars. But, if you're trying to use something like lexical search or embedding search, just by searching Oscar nominated films, we're not guaranteed that all those movies are actually Oscar nominated.

We might want to have specific datasets or acquire specific datasets that allow us to join on these kind of tags and then filter them in our search queries.

And then lastly, maybe it's around when the query was being asked.  If I search Chinese food and it's after 9pm,  the results are still pretty bad. Maybe it turns out that we just are missing a bunch of up to date availability data, and just by building something like an Open Now button to specify whether or not restaurants are open by the time they order something, we can actually have a much better user experience.

And again, none of these issues are necessarily about the AI itself, but you can imagine a lot of this being around. How do we add additional metadata? How can we add additional filters in your function calls? And how do we build specific features that matter to your customer and their use case.

Here, I just wanted to list out some common and very fixable capabilities issues that I faced with my clients. The first one is very obvious. It's around daytime filters, right? How do we figure out what is recently mean? How can we only search content that is a couple days old or a couple months old if they specify that?

The second one is comparisons what is the difference between 2022 and 2023? Oftentimes, I really care about having filters for tabular data that exists in a PDF, maybe specific filters for a stock ticker before a document is triggered, or even just understanding the document metadata directly.

For example, who edited this document last or when was this last modified?

For datetime filtering, if you use things like lancedb or Postgres, there's a lot of simple ways of just making sure you have a between clause, for example.

For comparisons, you have to make sure your language model has the ability to make multiple search queries, and then naturally the generation prompt can correctly reason about the two and produce a reasonable answer.

For tabular data, sometimes you might want to allow the language model to access something like running SQL, whereas other times just searching for the rows and columns in a large data set might be enough.

And just like filtering for date times, if you need to search for specific things like a stock ticker or a quarter or a document type, a lot of that is possible when you use libraries that explicitly allow metadata filtering.

And if you use something like full text search and elastic search, then you're able to query multiple aspects of the document metadata, like the modification history or the last person that was able to , do some kind of work.

And of course, if you have any questions on how we might want to implement these in a much more specific way, feel free to bring that up in the questions channel or on the Q& A section. On office hours.

And here are some of the more common things I've done with clients to improve this capabilities issue. A lot of it will be parallel function calling and query routing to make sure that we can combine parallel answers. Oftentimes it involves pre processing a lot of the data and building specific indices to search against or to filter or sort some of these queries.

And as language models get better and better, conditionally using things like long context rather than RAG when answers are found in short documents have produced really great results for a lot of use cases. And then lastly, we can also dynamically choose based on the question whether or not we want to change the generation prompt.

For example, when things are being compared.

I also want to call out the fact that many people might also already understand what topic modeling is for and what it does. But I just want to make a call out here that topic modeling is just a tool to eventually come up with explicit classes.

And I don't want you to overthink what kind of topic modeling you want to use. Generally, if you just use embeddings, K means. Once you build out these clusters and start looking at your data, you're going to understand what the inventory issues are, and you're going to understand what their capabilities are.

And once you do, you're going to go out and build a very simple classifier to run through your data and do this analysis. The topic model is not the end solution, it's just a tool that you can use to do data analysis.

And once you do this data analysis, and once you just look at, , dozens and dozens of topics and, dozens and dozens of questions. we're generally going to have a good understanding of what the capabilities are. And you're going to run these clusters, work with domain experts, and build some few shot classifiers, to find and propose new segments.

Sometimes I've even just taken the clusters and, a couple dozen examples per cluster and passed that into O1 Pro and have it give me a report on what kind of capabilities exist. And then I build my classifier.

And I've classified on things like what kind of question types might exist, what kind of context might be recovered, what kind of search indices might exist, and even the different kinds of response formats that might be required.

And again, feel free to take a screenshot of this, but you'll notice even subgroups, right? Whether or not a query contains something, whether the query requires something, whether the answer is a certain format. That's also just to help me understand what my customers are actually asking for.

And Anthropic recently, using a tool called Clio, did a similar analysis  on the kinds of conversations people are having with Clot. And here what you notice, for example, is that the computer and math section are dramatically over,

dramatically above baseline economic use. Along with things like art and design and a bunch of the life and social sciences.

And by doing this, you might realize, okay, maybe we should make sure that we continue to do very well in the computer and mathematics section. But potentially we want to make some better investments in some of the, uh,  the smaller markets. It really just depends on what you're trying to do for your business.

Monitoring and Feedback

So now let's talk a little  üìç  üìç bit more about monitoring.

If you don't have any data to begin with, it doesn't mean you can't do this right now. You might want to just create some hypothetical segments that you're concerned about or you're interested in understanding, and just set up a monitoring system as you roll out your production application.  

If you do have user data, then I recommend just running this topic model, finding some clusters, and just find maybe five or ten questions that are satisfactory or unsatisfactory for each cluster.

This might just be, ten examples of questions within a cluster that has a thumb up, maybe ten examples with a thumb down.  And then what you can do is you can just share this with a domain expert, with your user researchers or even just O1 Pro, and just have a conversation of, what do we think the inventory issues are?

And what do you think the capability issues are? Do we just lack the ability to do a time filter? Or is it much more complicated than that?

And maybe what you find is,  let's consider like a customer support example. Maybe what you find is through some data analysis that we're actually pretty good at just finding what's happening. Certain kinds of questions, and we're really good at just doing a user filter or a limit query.

Consider an example where we uncover some topics and we look at some positive examples. Show me the last 10 support tickets. First 10 support tickets about battery life complaints. Jason, lose support tickets. What this suggests is maybe we're already pretty good at finding,  top 10 results. with some very simple filter, and we can even do things like filter for a specific,  support agent.

But then what you're going to realize is you're going to  üìç  üìç get some really crazy questions, right? Then we're getting questions like , is Jason a good customer support support rep? Who is going to churn and why? Or what do people complain most about? And what you're going to realize is those questions might not be possible with a simple 

ticket chunking strategy. But if these are questions that are being asked very often, now we can look at, could we build a system that predicts churn? Could we build a system that has a,, a reputation or a score, , per individual? And can we build a system that does summarization so we can figure out what people complain about the most?

And none of this is impossible, but it's not going to be possible with a simple You're going to be just building out specific workloads that you've uncovered to be valuable.

 And it might be just the case that these solutions don't even require better models. Maybe we just need to render support tickets in the UI a lot better because we're asking questions about them so much. Maybe we already have separate dashboards for how good a customer support rep is and we just need to render those.

I just also want to make a specific call out It's likely to be the case that the first couple of times you do this it'll be an offline analysis  But it's going to be very valuable to just transfer this offline analysis to online analysis as well.

You really want to monitor the impact of your solutions in real time.

Because not only do you get immediate feedback, but you can also figure out how your users are adjusting their own behavior in response to your new solutions or your new implementations.

The general principle of why we want to monitor this stuff in is this thing I read in like an NDA book somewhere called the Automation Paradox. , and it states that automation saves you time, but the issues will multiply if left unchecked. If you have a machine punch holes and it's off by an inch, If you don't check this over time, it's going to ruin a bunch of your, a bunch of your production.

But the solution to this is just doing very high quality sampling,? As long as  üìç  üìç we're checking at a regular cadence, we're going to be okay. And we're going to be able to figure out what's going wrong, what's going well, and just understand what is the health of this system. And so what I really suggest is once we've identified these clusters, we can build classification models.

And as long as we have an other category that is well prompted, we can also start detecting changes in the system. Then you can do a bunch of different kinds of dashboards. You can track the distribution of query types over time. You could track the percentage of the other category to figure out if something new is happening behind the scenes.

Maybe we want to  üìç  üìç have other be at around 10 percent and if it goes above 10%, we might have to do this analysis again and figure out if there's something else that needs to be improved. Maybe we can group by the different query types and track the average cosine similarity or the average coherent ranker score To give us an idea of whether or not relevancy is good as an early way of predicting our inventory limitations.

We could even track these same metrics across different cohorts or organizations. Maybe one day you onboard Home Depot and the next day you onboard Walmart. Are the questions the same? Who knows? Is the relevancy the same? Who knows? But maybe one day you realize that Walmart hasn't been providing the same kind of richness of data as someone else and you can  determine that by doing a group by organization to find the average Cohere Ranker score.

All of these things are possible if you do this analysis. And this has happened plenty of times in the past, right? We've had situations where. , all of a sudden,  our P6S drops. We don't know why, we ask around, and in two days we find out, okay, it turns out  we launched a really ineffective marketing campaign.

And that's why we had brought in a bunch of users that we couldn't really sell to.

And like I said in this example, we're always trying to bring new users to the platform, we're always trying to introduce new features, and by just having a bunch of these time series data and these distributions, we're going to be able to better detect and understand why our customer satisfaction might drift, or why our percentages might drift.

And again, we're going to go back to those three variables, right? For every segment, there is the impact that we have, there is the, Probability of success that matters, and ultimately the volume of questions that matter.

And the goal that I want you to have  for this session, is to gain a holistic view of what system performance looks like,

and identify specific areas, whether it's the content or the features or the users that need the most attention,  and to make sure we tailor specific interventions to improve. and address the most impactful issues. And once we can do that, we can proactively monitor our system to detect evolving changes in our users and needs and behaviors.

The goal is not only to detect the concept drift, but to understand its nuances across multiple aspects of your  problem space.

Homework and Conclusion

And now let's just jump into some homework and food for thought for this session, and what I hope you can spend some time thinking about this week in terms of how you want to collect data, but also how you want to, might wanna communicate this to your, to your team.

So whether this is something that you're doing for your work or for your own projects, I really wanted to think about how we can internalize this with the team that you're on. What kind of topic modeling and batch classification can we do to identify our inventory and capability issues? Can we identify examples of questions we can answer well and questions we can answer poorly and just figure out what would need to change?

to make them a lot better? And then lastly, do we have some way to audit this kind of feedback? Do we have some kind of user feedback UI? Do we have some way of collecting the thumbs up and thumbs down labels? Do we have some way of monitoring  üìç  üìç this data? And once we do this analysis, really ask yourself, could we have done a lot better if we had better metadata?

Or if we were able to filter on certain things. Or if we were to build specific indices that might not exist currently. Maybe we just need to build a better image search tool. Or a better calendar search tool. If we could do that separately, we could do a lot better.

And the thing I really want to stick into your mind today, is understanding the next time someone says make the AI better, I want you to think about what specific kinds of query. query types we might need to identify to double down on with the goal that potentially the next time we are to stand up instead of just  coming back and just saying we need to make the AI better we can identify which segment we need to improve by how much and what kind of experiments we need to get that done 



So that's basically it for this week. I know this video has been a little bit longer than the previous ones,  but as always, if you have any questions, please ask that in the questions slack. We have plenty of office hours,  and more importantly, in this session's tutorials, we'll go over examples of how we can think about doing question generation, clustering, as well as classification.

